{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarefa 2: Classificação Binária de Empréstimos\n",
    "\n",
    "Este notebook segue os passos de 'a' até 'i' da avaliação:\n",
    "1.  Carregar e preparar dados (split, remoção de colunas, encoding, etc.).\n",
    "2.  Construir um `ColumnTransformer` robusto para pré-processamento.\n",
    "3.  Usar `imblearn.pipeline.Pipeline` para integrar o pré-processamento, `SMOTE` e o classificador.\n",
    "4.  Definir e executar `GridSearchCV` para 5 algoritmos diferentes.\n",
    "5.  Comparar os modelos usando `classification_report` e `roc_auc_score`.\n",
    "6.  Salvar o pipeline final e completo (com pré-processamento e modelo) usando `joblib`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Célula 1: Importação das Bibliotecas\n",
    "\n",
    "Importamos `pandas` e `numpy` para dados, e diversas ferramentas do `sklearn`: `train_test_split`, pré-processadores (`StandardScaler`, `OneHotEncoder`, `SimpleImputer`), `ColumnTransformer` para organizar o pré-processamento, e as métricas de avaliação. \n",
    "\n",
    "Também importamos `SMOTE` (da biblioteca `imbalanced-learn`) para lidar com o desbalanceamento de classe (Instrução 2b) e a versão do `Pipeline` do `imblearn` que é compatível com o SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pipelines e Modelos\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Célula 2: Etapas 2a-f (Carga e Preparação Inicial)\n",
    "\n",
    "1.  **Carregar Dados:** Baixamos o `loan.csv` do GitHub.\n",
    "2.  **Remover Colunas (2b):** Removemos as colunas solicitadas.\n",
    "3.  **Mapear Target (2e):** Convertemos a coluna alvo `Loan_Status` de 'Y'/'N' para 1/0.\n",
    "4.  **Separar X e y:** Isolamos a variável alvo (`y`) das features (`X`).\n",
    "5.  **Dividir Treino/Teste (2a):** Dividimos os dados em 80% para treino e 20% para teste (`random_state=42`). Usamos `stratify=y` para garantir que a proporção de classes (Sim/Não) seja a mesma nos dois conjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Carregar Dados\n",
    "data_url = 'https://raw.githubusercontent.com/josenalde/machinelearning/main/datasets/loan.csv'\n",
    "df = pd.read_csv(data_url)\n",
    "\n",
    "# 2. Remover Colunas (Instrução 2b)\n",
    "cols_to_drop = ['Loan_ID', 'CoapplicantIncome', 'Loan_Amount_Term', 'Credit_History', 'Property_Area']\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# 4. Mapear Target (Instrução 2e)\n",
    "df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})\n",
    "df = df.dropna(subset=['Loan_Status']) # Remover linhas onde o alvo é nulo\n",
    "\n",
    "# 5. Separar X e y\n",
    "y = df['Loan_Status']\n",
    "X = df.drop('Loan_Status', axis=1)\n",
    "\n",
    "# 6. Separar Treino e Teste (Instrução 2a)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Balanceamento do Target (Treino) antes do SMOTE:\")\n",
    "print(y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Célula 3: Função Auxiliar para 'Dependents'\n",
    "\n",
    "**Etapa 2f:** A instrução pede para mapear '3+' para '3'. Criamos uma função `map_dependents` que faz exatamente isso. Usaremos `FunctionTransformer` para incluir esta etapa personalizada dentro do pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Mapear 'Dependents' (Instrução 2f)\n",
    "# Esta transformação será colocada em um FunctionTransformer dentro do pipeline\n",
    "def map_dependents(X_in):\n",
    "    # X_in será um DataFrame (ou array) apenas com a coluna 'Dependents'\n",
    "    X = pd.DataFrame(X_in, columns=['Dependents'])\n",
    "    X['Dependents'] = X['Dependents'].astype(str).replace('3+', '3')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Célula 4: Definição dos Pipelines de Pré-processamento\n",
    "\n",
    "Esta é a parte central do pré-processamento. Usamos `ColumnTransformer` para aplicar diferentes transformações a diferentes colunas:\n",
    "\n",
    "1.  **`numeric_transformer` (Etapa 2g):**\n",
    "    * Para `ApplicantIncome` e `LoanAmount`.\n",
    "    * Usa `SimpleImputer(strategy='median')` para preencher valores faltantes (NaN) com a mediana.\n",
    "    * Usa `StandardScaler` para padronizar os dados.\n",
    "\n",
    "2.  **`categorical_transformer` (Etapas 2c, 2e):**\n",
    "    * Para `Gender`, `Married`, `Education`, `Self_Employed`.\n",
    "    * Usa `SimpleImputer(strategy='most_frequent')` para preencher NaNs com a moda (valor mais comum).\n",
    "    * Usa `OneHotEncoder` para converter as categorias (ex: 'Male'/'Female') em colunas numéricas (0s e 1s).\n",
    "\n",
    "3.  **`dependents_transformer` (Etapas 2c, 2f):**\n",
    "    * Apenas para `Dependents`.\n",
    "    * Imputa NaNs com a moda (2c).\n",
    "    * Aplica a função `map_dependents` para tratar o '3+' (2f).\n",
    "    * Usa `OneHotEncoder` para converter '0', '1', '2', '3' em colunas numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Definir Pipelines de Pré-processamento (Instruções 2c, 2e, 2g)\n",
    "\n",
    "# Features numéricas para escalar (2g)\n",
    "numeric_features = ['ApplicantIncome', 'LoanAmount']\n",
    "numeric_transformer = ImbPipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')), # Adicionando imputer para robustez\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Features categóricas para imputar com moda (2c) e codificar (2e)\n",
    "# Usamos OneHotEncoder (em vez de LabelEncoder) pois é o padrão para features nominais em ML\n",
    "categorical_features = ['Gender', 'Married', 'Education', 'Self_Employed']\n",
    "categorical_transformer = ImbPipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Feature 'Dependents' com tratamento especial (2c, 2f)\n",
    "dependents_feature = ['Dependents']\n",
    "dependents_transformer = ImbPipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # Imputa com moda (ex: '0')\n",
    "    ('mapper', FunctionTransformer(map_dependents)), # Mapeia '3+' para '3'\n",
    "    # Especifica as categorias para garantir a ordem correta no OneHotEncoder\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', categories=[['0', '1', '2', '3']], sparse_output=False))\n",
    "])\n",
    "\n",
    "# Montar o ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('dep', dependents_transformer, dependents_feature)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Célula 5: Etapa 2h - Definição dos 5 Pipelines e Grids de Busca\n",
    "\n",
    "Definimos os 5 algoritmos de classificação e os hiperparâmetros que queremos testar para cada um deles. Os nomes no grid (ex: `classifier__C`) referem-se aos passos que definiremos no pipeline da próxima célula (`smote` e `classifier`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos para testar\n",
    "models_to_search = {\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, solver='liblinear'),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'KNeighbors': KNeighborsClassifier(),\n",
    "    'SVC': SVC(probability=True, random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Grids de Hiperparâmetros (reduzidos para velocidade)\n",
    "param_grids = {\n",
    "    'LogisticRegression': {\n",
    "        'classifier__C': [0.1, 1.0, 10],\n",
    "        'classifier__penalty': ['l1', 'l2']\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__max_depth': [5, 10, None]\n",
    "    },\n",
    "    'KNeighbors': {\n",
    "        'classifier__n_neighbors': [5, 7, 9]\n",
    "    },\n",
    "    'SVC': {\n",
    "        'classifier__C': [0.1, 1.0],\n",
    "        'classifier__kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__learning_rate': [0.05, 0.1]\n",
    "    }\n",
    "}\n",
    "\n",
    "best_estimators = {}\n",
    "\n",
    "print(\"Iniciando GridSearch para 5 modelos...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Célula 6: Etapa 2h (Execução) - GridSearchCV\n",
    "\n",
    "Este é o loop principal de treinamento. Para cada um dos 5 modelos:\n",
    "1.  **Cria um `ImbPipeline`:**\n",
    "    * `preprocessor`: O `ColumnTransformer` que definimos (trata NaNs, '3+', escala e codifica).\n",
    "    * `smote`: O `SMOTE` (Instrução 2b) para balancear os dados *apenas no treino* (o pipeline aplica isso internamente em cada fold do CV).\n",
    "    * `classifier`: O modelo sendo testado (ex: `LogisticRegression`).\n",
    "2.  **Cria o `GridSearchCV`:** Passamos o pipeline e o grid de parâmetros correspondente. Usamos `cv=5` (validação cruzada de 5 folds) e `scoring='roc_auc'` (como solicitado).\n",
    "3.  **Executa `grid_search.fit`:** O grid search treina e avalia todas as combinações de parâmetros no conjunto `X_train`, `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models_to_search.items():\n",
    "    print(f\"\\n--- Buscando para {name} ---\")\n",
    "    \n",
    "    # Criar o pipeline completo com SMOTE (Instrução 2b)\n",
    "    pipeline = ImbPipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, \n",
    "        param_grid=param_grids[name], \n",
    "        cv=5, \n",
    "        scoring='roc_auc', # Comparando por AUC como solicitado\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_estimators[name] = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"Melhor Score (AUC) CV: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"Melhores Parâmetros: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Célula 7: Etapa 2h (Avaliação) - Comparação no Teste\n",
    "\n",
    "Avaliamos o melhor pipeline (`best_estimator_`) de cada um dos 5 modelos no conjunto de teste (`X_test`, `y_test`), que foi mantido separado durante todo o processo.\n",
    "\n",
    "Imprimimos o **AUC** e o **Classification Report** (precision, recall, f1-score) para cada um, e plotamos a Matriz de Confusão para análise visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Avaliação dos Melhores Modelos no Conjunto de Teste ---\")\n",
    "best_model_name = None\n",
    "best_model_auc = 0.0\n",
    "final_pipeline_to_save = None\n",
    "\n",
    "for name, model_pipeline in best_estimators.items():\n",
    "    y_pred = model_pipeline.predict(X_test)\n",
    "    y_proba = model_pipeline.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    print(f\"\\n========== {name} ==========\")\n",
    "    print(f\"AUC no Teste: {auc:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negado', 'Aprovado'])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f\"Matriz de Confusão - {name}\")\n",
    "    plt.show()\n",
    "    \n",
    "    if auc > best_model_auc:\n",
    "        best_model_auc = auc\n",
    "        best_model_name = name\n",
    "        final_pipeline_to_save = model_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Célula 8: Etapa 2i - Salvando o Modelo Final\n",
    "\n",
    "Selecionamos o pipeline que teve o melhor AUC no conjunto de teste e o salvamos em `loan_pipeline.joblib`. Este arquivo único contém todas as etapas: pré-processamento (imputação, scaling, one-hot-encoding) e o modelo classificador já treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'loan_pipeline.joblib'\n",
    "joblib.dump(final_pipeline_to_save, model_filename)\n",
    "\n",
    "print(f\"\\n--- Etapa 2i Concluída ---\")\n",
    "print(f\"Melhor modelo: {best_model_name} (AUC: {best_model_auc:.4f})\")\n",
    "print(f\"Pipeline final salvo em: {model_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}